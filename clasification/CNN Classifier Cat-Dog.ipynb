{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las liobrerías y paquetes\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'deeplearning-az/datasets/Part 2 - Convolutional Neural Networks (CNN)/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 62, 62, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 29, 29, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 2,456,513\n",
      "Trainable params: 2,455,041\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Inicializar la CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Paso 1 - Seccion Convolución\n",
    "classifier.add(Conv2D(filters = 32, kernel_size = (3, 3), input_shape = (64, 64, 3), activation = \"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "# Una segunda capa de convolución\n",
    "classifier.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = \"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "# Una tercera capa de convolución\n",
    "classifier.add(Conv2D(filters = 128, kernel_size = (3, 3), activation = \"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "# Paso 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Paso 4 - Full Connection\n",
    "classifier.add(Dense(units = 512, activation = \"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compilar la CNN\n",
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Parte 2 - Ajustar la CNN a las imágenes para entrenar \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        #rescale=1./255,\n",
    "        #shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        #horizontal_flip=True\n",
    "        rotation_range=15,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_dataset = train_datagen.flow_from_directory(base_path + 'dataset/training_set',\n",
    "                                                    target_size=(64, 64),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "testing_dataset = test_datagen.flow_from_directory(base_path + 'dataset/test_set',\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir callback para reducir learning rate\n",
    "\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR) :\n",
    "    os.mkdir(MODEL_DIR)\n",
    "modelpath = './model/dogcat-{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "checkPointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "earlystop = EarlyStopping(patience=10 )\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience=2,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.5,\n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "callbacks=[earlystop, learning_rate_reduction, checkPointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 289s 36ms/step - loss: 0.4502 - accuracy: 0.7851 - val_loss: 0.3599 - val_accuracy: 0.8198\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35991, saving model to ./model/dogcat-01-0.3599.hdf5\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 287s 36ms/step - loss: 0.3133 - accuracy: 0.8633 - val_loss: 0.1980 - val_accuracy: 0.8884\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35991 to 0.19799, saving model to ./model/dogcat-02-0.1980.hdf5\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 287s 36ms/step - loss: 0.2714 - accuracy: 0.8838 - val_loss: 0.2879 - val_accuracy: 0.8857\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.19799\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 285s 36ms/step - loss: 0.2409 - accuracy: 0.8985 - val_loss: 0.1736 - val_accuracy: 0.9139\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.19799 to 0.17361, saving model to ./model/dogcat-04-0.1736.hdf5\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 287s 36ms/step - loss: 0.2237 - accuracy: 0.9076 - val_loss: 0.1904 - val_accuracy: 0.8805\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.17361\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.2088 - accuracy: 0.9140 - val_loss: 0.4302 - val_accuracy: 0.9008\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.17361\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1776 - accuracy: 0.9282 - val_loss: 0.3496 - val_accuracy: 0.9061\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.17361\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1647 - accuracy: 0.9336 - val_loss: 0.2526 - val_accuracy: 0.9081\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.17361\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1494 - accuracy: 0.9408 - val_loss: 0.0205 - val_accuracy: 0.9146\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.17361 to 0.02049, saving model to ./model/dogcat-09-0.0205.hdf5\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1418 - accuracy: 0.9438 - val_loss: 0.1268 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02049\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1366 - accuracy: 0.9458 - val_loss: 0.1374 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02049\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 285s 36ms/step - loss: 0.1337 - accuracy: 0.9473 - val_loss: 0.1825 - val_accuracy: 0.9125\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02049\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1281 - accuracy: 0.9495 - val_loss: 0.6460 - val_accuracy: 0.9095\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02049\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1231 - accuracy: 0.9520 - val_loss: 0.1069 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02049\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1196 - accuracy: 0.9535 - val_loss: 0.4290 - val_accuracy: 0.9089\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02049\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 285s 36ms/step - loss: 0.1147 - accuracy: 0.9553 - val_loss: 0.1853 - val_accuracy: 0.9114\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02049\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 287s 36ms/step - loss: 0.1135 - accuracy: 0.9564 - val_loss: 0.3197 - val_accuracy: 0.9115\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02049\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1128 - accuracy: 0.9565 - val_loss: 0.4826 - val_accuracy: 0.9128\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02049\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 286s 36ms/step - loss: 0.1104 - accuracy: 0.9576 - val_loss: 0.4406 - val_accuracy: 0.9128\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f7fb46cbef0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar\n",
    "\n",
    "classifier.fit_generator(\n",
    "                        training_dataset,\n",
    "                        steps_per_epoch=8000,\n",
    "                        epochs=30,\n",
    "                        validation_data=testing_dataset,\n",
    "                        validation_steps=2000,\n",
    "                        callbacks=callbacks,\n",
    "                        use_multiprocessing=True,\n",
    "                        workers=16)\n",
    "\n",
    "#classifier.save('model_dogcat_03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': 0, 'dogs': 1}\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "# Parte 3 - Cómo hacer nuevas predicciones\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# Load model\n",
    "classifier = keras.models.load_model('./model/dogcat-09-0.0205.hdf5')\n",
    "\n",
    "\n",
    "test_image = image.load_img(base_path + 'dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "print(training_dataset.class_indices)\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Accuracy =  0.9148185\n"
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TEST = testing_dataset.n // testing_dataset.batch_size\n",
    "result = classifier.evaluate_generator(generator=testing_dataset, steps=STEP_SIZE_TEST)\n",
    "print(\"Accuracy = \",result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
